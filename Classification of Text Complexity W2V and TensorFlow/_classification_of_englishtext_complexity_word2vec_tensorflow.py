# -*- coding: utf-8 -*-
"""-classification-of-englishtext-complexity-word2vec-tensorflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/MariaPetrovskaya/bd911353d2259039719df3a1f92855a1/-classification-of-englishtext-complexity-word2vec-tensorflow.ipynb

#About project

###Introduction

*   Watching films in the original language is a popular and effective method  for training languages skills.
*   It is important to choose a film that suits actual level of difficulty, (possible to understood 50-70% of the dialogues).
*   To fulfill this condition, the expert must watch the new film and decide what level it corresponds to. However, this requires a lot of time.


###Business task

*   Customer of this product: the foreign language courses.
*   Develop an ML solution (multi-class classification) for prediction of difficulty levels for English-language films.

###Initial data
*   The study was experimental stage, dataset was of limited size (200 + records).
*   Subtitles of films saved in directories with names as level of difficulty according to the CEFR ([Common European Framework of Reference]('https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%89%D0%B5%D0%B5%D0%B2%D1%80%D0%BE%D0%BF%D0%B5%D0%B9%D1%81%D0%BA%D0%B8%D0%B5_%D0%BA%D0%BE%D0%BC%D0%BF%D0%B5%D1%82%D0%B5%D0%BD%D1%86%D0%B8%D0%B8_%D0%B2%D0%BB%D0%B0%D0%B4%D0%B5%D0%BD%D0%B8%D1%8F_%D0%B8%D0%BD%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%BD%D1%8B%D0%BC_%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%BC' 'wikipedia')).
*   Movie subtitles in XLSX files containing the name of the movies and the degree of difficulty according to the CEFR scale.

---
"""

! pip install gensim # better to install this library first, if work in colab

# import modules & set up logging
import gensim, logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

import multiprocessing

from gensim.models import Word2Vec

from gensim import models

from gensim.utils import keep_vocab_item, call_on_class_only, deprecated
from gensim.models.keyedvectors import KeyedVectors, pseudorandom_weak_vector
from gensim import utils, matutils

# This import is required by pickle to load models stored by Gensim < 4.0, such as Gensim 3.8.3.
from gensim.models.keyedvectors import Vocab  # noqa

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # need to install in colab
# !pip install pysrt
# 
# !pip install tabula-py
#

import tensorflow as tf

# Commented out IPython magic to ensure Python compatibility.
from string import punctuation

from collections import Counter
import matplotlib.pyplot as plt

from IPython.display import Image
from IPython.core.display import HTML
# %matplotlib inline

# imports
import os
import re

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import (train_test_split,
                                     StratifiedShuffleSplit,
                                     GridSearchCV
                                     )


from sklearn.metrics import (classification_report,
                             f1_score,
                             confusion_matrix,
                             ConfusionMatrixDisplay
                             )

from sklearn.base import (BaseEstimator,
                          TransformerMixin
                          )
import pickle
import pysrt

import requests
import urllib
import json
import zipfile
from zipfile import ZipFile
import tabula
from tabula import read_pdf

"""# Data Loading and Preprocessing

###Loadind data with movies
"""

#import from с Google Disk to Colab
from google.colab import drive
drive.mount('/content/drive')

PATH = '/content/drive/MyDrive/datasets'
path_to_zip_file = f'{PATH}/English_scores.zip'
path_to_zip_file

with ZipFile(path_to_zip_file, 'r') as zObject:
  znm = zObject.namelist()

with ZipFile(path_to_zip_file, 'r') as zObject:
  movies_labels = zObject.extract('movies_labels.xlsx')

#loading dataset with movies levels
df = pd.read_excel(movies_labels, index_col='id')
df.info

# looking list of levels
df['Level'].unique()

# assign a numeric values to the levels
label_dict = {'A2': 1,
              'A2/A2+': 1,
              'B1': 2,
              'A2/A2+, B1': 2,
              'B2': 3,
              'B1, B2': 3,
              'C1': 4}
# change levels to numeric values
df['Level'] = df['Level'].replace(label_dict)

# drop duplicates
df = df.drop_duplicates()
df.shape

"""###Loading subtitles"""

films_name = {}
for srt in znm:
  if srt.find('.srt') != -1 and srt.find('__MACOSX') == -1:
    films_name[srt.split('/')[-1]] = srt
print(len(films_name))

# checking number of films from table with subtitles
films_filtr = set(films_name.keys()) & set(df['Movie'] + '.srt')
len(films_filtr)

def load_subs(film):
  """loading subtitles function"""
  fsrt = f'{film}.srt'
  if fsrt not in set(films_name.keys()):
    return None
  with ZipFile(path_to_zip_file, 'r') as zObject:
    f = zObject.extract(films_name[fsrt])
    try:
      subs = pysrt.open(f)
    except:
      subs = pysrt.open(f,encoding='iso-8859-1')
  return subs

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #loading subtitles
# df['subs'] = df['Movie'].apply(load_subs)

df['subs']

#dropping gaps
df = df.dropna().reset_index(drop=True)

df

"""###Data imbalances"""

df['Level'].value_counts()

"""##Classification task

###Subtitle text preprocessing
"""

# import re
def preprocess(text):
    tokens = re.sub('#+', ' ', text.lower()).split()
    tokens = [token.strip(punctuation) for token in tokens]
    tokens = [token for token in tokens if token]
    return tokens

"""При рассмотрении текстов после лемматизации были обнаружены лишние токены (например _x000В_), поэтому добавляется дополнительная функция обработки текста с помощью регулярных выражений."""

# set regular expressions for text preprocessing
HTML = r'<.*?>' # html tags
#TAG = r'{.*?}' # change tags to spaces
COMMENTS = r'[\(\[][A-Za-z ]+[\)\]]' # comments in parentheses
LETTERS = r'[^\w\s]' # anything that isn't a letter
SPACES = r'([ ])\1+' # repeated spaces
DOTS = r'[\.]+' # punctuation mark ellipsis
SYMB = r"[^\w\d'\s]" # punctuation marks other than apostrophe
def clean_subs(subs):
  """subtitle text preprocessing"""
  subs = subs[1:] # remove the first promotional subtitle
  txt = re.sub(HTML, ' ', subs.text) # html tags change to spaces
  # txt = re.sub(COMMENTS, ' ', txt) # comments in parentheses change to spaces
  # txt = re.sub(LETTERS, ' ', txt) # anything that isn't a letter is a space
  # txt = re.sub(DOTS, r'.', txt) # punctuation mark ellipsis change to dots
  # txt = re.sub(SPACES, r'\1', txt) # repeated spaces change to one space
  # txt = re.sub(SYMB, '', txt) # punctuation marks other than apostrophe change to an empty string
  # txt = re.sub('www', '', txt) # 'www' change to an empty string
  # txt = txt.lstrip().rstrip().replace("\n","") # left space trimming
  # txt = txt.encode('ascii', 'ignore').decode() # remove all non-ascii characters
  #txt = txt.lower() # to lowercase
  txt = ' '.join(txt.split())
  return txt

df

df['Level'].info()

df['clean_text'] = df.subs.apply(clean_subs)

df['norm_text'] = df.clean_text.apply(preprocess)

df

"""### Word2Vec for embedding

Word vectors: will use **Word2Vec** to train word embeddings, using **gensim**,   NLP library that features many vector-based models incuding word2vec.

The input must be a tokenized text
"""

all_text = ' '.join(df['clean_text'])

all_text

texts = [preprocess(text) for text in df['clean_text']]

texts

gensim.__version__

"""It is necessary to transfer the corpus for training to model; for other parameters there are default values."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# w2v = gensim.models.Word2Vec(texts, )

"""To check the model work"""

w2v.wv.most_similar('future')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# w2v = gensim.models.Word2Vec(texts,
#                              vector_size=300,
#                              min_count=30,
#                              max_vocab_size=10000,
#                              window=5,
#                              epochs=7)

w2v.wv.most_similar('future')

"""At next step we collect the dictionary"""

vocab = Counter()

for text in df['norm_text']:
    vocab.update(text)

filtered_vocab = set()

for word in vocab:
    if vocab[word] > 5:
        filtered_vocab.add(word)

len(filtered_vocab)

word2id = { 'PAD':0}

for word in filtered_vocab:
    word2id[word] = len(word2id)
id2word = {i:word for word, i in word2id.items()}

"""Next step is to set indexes for tokens"""

X = []

for tokens in df['norm_text']:
    ids = [word2id[token] for token in tokens if token in word2id]
    X.append(ids)

"""###Splitting data into training and test samples"""

X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=100)
y = df.Level.values

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05)

"""###TensorFlow model"""

inputs = tf.keras.layers.Input(shape=(100,))

embeddings = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=100)(inputs, )
# embedding layer returns a sequence of vectors
# in this task we need to classify all text at once
# for this will get mean of single vectors as 1 text vector
mean = tf.keras.layers.Lambda(lambda x: tf.keras.backend.mean(x,  axis=1))(embeddings)
# mean vector, fully connected layer, return the class probability
outputs = tf.keras.layers.Dense(5, activation='softmax')(mean)

model = tf.keras.Model(inputs=inputs, outputs=outputs)
optimizer = tf.keras.optimizers.Adam()
model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])

model.fit(X_train, y_train,
          validation_data=(X_valid, y_valid),
          batch_size=32,
         epochs=10)

"""In this model, vector representations are training from scratch.  To make it better, we can add pretrained vectors from word2vec to the embedding layer"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ft = gensim.models.Word2Vec(texts + df['norm_text'].values.tolist(), window=10, epochs=200) #epochs=1000 same result

"""Forming the matrix with vectors. The indices of the rows in this matrix are similar with the indices of the words in the dictionary."""

weights = np.zeros((len(word2id), 100))

for word, i in word2id.items():
    # padding vector as zero
    if word == 'PAD':
        continue

    try:
        weights[i] = ft.wv[word]


    except KeyError:
        # for words that are not in the models, set some random vector
        continue
        weights[i] = ft.wv['rwwatandomhm']

inputs = tf.keras.layers.Input(shape=(100,))

# enter matrix to the embedding layer
# set trainable=False, so that vectors don’t learn
# Note: if this is not done, embedding will learn again

embeddings = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=100,
                                       trainable=False,
                                       weights=[weights])(inputs, )
mean = tf.keras.layers.Lambda(lambda x: tf.keras.backend.mean(x,  axis=1))(embeddings)

outputs = tf.keras.layers.Dense(5, activation='softmax')(mean)

model = tf.keras.Model(inputs=inputs, outputs=outputs)
optimizer = tf.keras.optimizers.Adam()
model.compile(optimizer=optimizer,
              loss='SparseCategoricalCrossentropy',
              metrics=['sparse_categorical_accuracy'])

model.fit(X_train, y_train,
          validation_data=(X_valid, y_valid),
          batch_size=32,
         epochs=30)

print(model.history.history.keys())
# summarize history for accuracy
plt.plot(model.history.history['sparse_categorical_accuracy'])
plt.plot(model.history.history['val_sparse_categorical_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""The resulting metric sparse_categorical_accuracy = 0,75 is not bad for a small experimental dataset (230 items)

#Summary results

#### Conclusions
- The project is completed.
- Data was loaded and unpacked.
- Text data was preprocessed by .re (Regular expression operations).
- Word2Vec embeddings were trained using gensim library.
- A solution with TensorFlow has been implemented  ( using pretrained Word2Vec embeddings and also without them).
- The sparse_categorical_accuracy metric on validation was 0,75.
- To build a model with better metrics and practical application, a larger dataset is desirable.
"""

